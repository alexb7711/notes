<!DOCTYPE html>
<html>
<head>
    <title>3-introduction-to-keras-and-tensorflow</title>
    <link href="style.css" rel="stylesheet">
</head>
<body>
<div id="navigation">
<ul>
<li><a class="%blog.html%" href="blog.html">blog</a></li>
<li><a class="%notes.html%" href="notes.html">notes</a></li>
<li><a class="%projects.html%" href="projects.html">projects</a></li>
<li><a class="%index.md%" href="index.html">index</a></li>
</ul>
</div>

<div class="content">
<p>This chapter is meant to give you everything you need to start doing deep learning in practice.</p>
<h1 id="whats-tensorflow">Whats TensorFlow?</h1>
<p>TensorFlow is a Python-based, free, open source machine learning platform, developed primarily by Google. Much like
NumPy, the primary purpose of TensorFlow is to enable engineers and researchers to manipulate mathematical expressions
over numerical tensors. But TensorFlow goes far beyond the scope of NumPy in the following ways:</p>
<ul>
<li>It can automatically compute the gradient of any differentiable expression (as you saw in chapter 2), making it highly
  suitable for machine learning.</li>
<li>It can run not only on CPUs, but also on GPUs and TPUs, highly parallel hardware accelerators.</li>
<li>Computation defined in TensorFlow can be easily distributed across many machines.</li>
</ul>
<p>TensorFlow programs can be exported to other runtimes, such as C++, JavaScript (for browser-based applications), or
TensorFlow Lite (for applications running on mobile devices or embedded devices), etc. This makes TensorFlow
applications easy to deploy in practical settings.</p>
<h1 id="whats-keras">What's Keras?</h1>
<p>Keras is a deep learning AIP for Python, built on top of TenserFlow, that provides a convenient way to define and train
any kind of deep learning model.</p>
<p>Keras is known for prioritizing the developer experience. Itâ€™s an API for human beings, not machines. It follows best
practices for reducing cognitive load: it offers consistent and simple workflows, it minimizes the number of actions
required for common use cases, and it provides clear and actionable feedback upon user error. This makes Keras easy to
learn for a beginner, and highly productive to use for an expert.</p>
<h1 id="first-steps-with-tensorflow">First Steps With TensorFlow</h1>
<p>A significant difference between NumPy and TensorFlow is that TensorFlow tensors are not assignable, they are constant.
To update the value of a tensor, you have to use the assign method:</p>
<pre class="codehilite"><code class="language-python">v.assign(tf.ones((3, 1)))
v[0, 0].assign(3.)
</code></pre>

<h2 id="a-second-look-at-the-gradienttape-api">A Second Look At The GradientTape API</h2>
<p>We established earlier that although NumPy and TensorFlow have a lot of similar operations, one thing that NumPy can't
do is take the gradient of the result with respect to the inputs:</p>
<pre class="codehilite"><code class="language-python">input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
   result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
</code></pre>

<p>Gradient tape is also able to compute second-order gradients:</p>
<pre class="codehilite"><code class="language-python">time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position =  4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
</code></pre>

<h2 id="an-end-to-end-example-a-linear-classifier-in-pure-tenserflow">An End-To-End Example: A Linear Classifier In Pure TenserFlow</h2>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt

learning_rate = 0.1

def model(inputs):
    &quot;&quot;&quot;Forward pass function&quot;&quot;&quot;
    return tf.matmul(inputs, W) + b

def square_loss(targets, predictions):
    &quot;&quot;&quot;Mean squared error loss function&quot;&quot;&quot;
    per_sample_losses = tf.square(targets - predictions)
    return tf.reduce_mean(per_sample_losses)

def training_step(inputs, targets):
    &quot;&quot;&quot;One step when training the linear classifier variables&quot;&quot;&quot;
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(predictions, targets)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss

num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean=[0, 3],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
positive_samples = np.random.multivariate_normal(
    mean=[3, 0],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)

# Serialize the data
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)

# Used to apply labels in a data set to distingnuish data types
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=&quot;float32&quot;),
                     np.ones((num_samples_per_class, 1), dtype=&quot;float32&quot;)))

plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])
plt.show()

# Begin creating the linear classifier variables
input_dim = 2
output_dim = 1
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))

# Batch training loop
for step in range(40):
    loss = training_step(inputs, targets)
    print(f&quot;Loss at step {step}: {loss:.4f}&quot;)

# Plot the results
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] &gt; 0.5)
</code></pre>

<h1 id="anatomy-of-a-neural-network-understanding-core-keras-apis">Anatomy Of A Neural Network: Understanding Core Keras APIs</h1>
</div>
<footer>
<p>Generated by Pymind: 2025-07-26</p>
</footer>

</body>
</html>
